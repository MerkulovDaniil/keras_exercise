{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2 compat\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "ML_100K_FILENAME = ML_100K_URL.rsplit('/', 1)[1]\n",
    "ML_100K_FOLDER = 'ml-100k'\n",
    "\n",
    "if not op.exists(ML_100K_FILENAME):\n",
    "    print('Downloading %s to %s...' % (ML_100K_URL, ML_100K_FILENAME))\n",
    "    urlretrieve(ML_100K_URL, ML_100K_FILENAME)\n",
    "\n",
    "if not op.exists(ML_100K_FOLDER):\n",
    "    print('Extracting %s to %s...' % (ML_100K_FILENAME, ML_100K_FOLDER))\n",
    "    ZipFile(ML_100K_FILENAME).extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id       item_id        rating     timestamp\n",
      "count  90570.000000  90570.000000  90570.000000  9.057000e+04\n",
      "mean     461.494038    428.104891      3.523827  8.835073e+08\n",
      "std      266.004364    333.088029      1.126073  5.341684e+06\n",
      "min        1.000000      1.000000      1.000000  8.747247e+08\n",
      "25%      256.000000    174.000000      3.000000  8.794484e+08\n",
      "50%      442.000000    324.000000      4.000000  8.828143e+08\n",
      "75%      682.000000    636.000000      4.000000  8.882049e+08\n",
      "max      943.000000   1682.000000      5.000000  8.932866e+08\n",
      "           user_id      item_id       rating     timestamp\n",
      "count  9430.000000  9430.000000  9430.000000  9.430000e+03\n",
      "mean    472.000000   400.800954     3.587805  8.837354e+08\n",
      "std     272.234934   306.859789     1.120240  5.360562e+06\n",
      "min       1.000000     1.000000     1.000000  8.747247e+08\n",
      "25%     236.000000   182.000000     3.000000  8.794515e+08\n",
      "50%     472.000000   303.000000     4.000000  8.833904e+08\n",
      "75%     708.000000   566.000000     4.000000  8.886378e+08\n",
      "max     943.000000  1664.000000     5.000000  8.932866e+08\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(op.join(ML_100K_FOLDER, 'ua.base'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "data_test = pd.read_csv(op.join(ML_100K_FOLDER, 'ua.test'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "print(data_train.describe())\n",
    "print(data_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1        1       5  874965758\n",
       "1        1        2       3  876893171\n",
       "2        1        3       4  878542960\n",
       "3        1        4       3  876893119\n",
       "4        1        5       3  889751712"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=944, n_items=1683\n"
     ]
    }
   ],
   "source": [
    "max_user_id = max(data_train['user_id'].max(), data_test['user_id'].max())\n",
    "max_item_id = max(data_train['item_id'].max(), data_test['item_id'].max())\n",
    "\n",
    "n_users = max_user_id + 1\n",
    "n_items = max_item_id + 1\n",
    "\n",
    "print('n_users=%d, n_items=%d' % (n_users, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0        1        1       5  874965758\n",
      "2        1        3       4  878542960\n",
      "5        1        6       5  887431973\n",
      "6        1        7       4  875071561\n",
      "8        1        9       5  878543541\n",
      "   user_id  item_id  rating  timestamp\n",
      "0        1       20       4  887431883\n",
      "1        1       33       4  878542699\n",
      "2        1       61       4  878542420\n",
      "5        1      160       4  875072547\n",
      "6        1      171       5  889751711\n"
     ]
    }
   ],
   "source": [
    "pos_data_train = data_train.query(\"rating >= 4\")\n",
    "pos_data_test = data_test.query(\"rating >= 4\")\n",
    "\n",
    "print(pos_data_train.head())\n",
    "print(pos_data_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data_train['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data_test['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)\n",
    "\n",
    "def cos_mode(inputs):\n",
    "    \"\"\"Work around for Keras bug with merge([...], mode='cos').\n",
    "    \n",
    "    Compute the cosine similarity of two unormalized embeddings.\n",
    "    \"\"\"\n",
    "    latent_codes_1, latent_codes_2 = inputs\n",
    "    sq_norm_1 = tf.reduce_sum(latent_codes_1 ** 2, axis=-1)\n",
    "    sq_norm_2 = tf.reduce_sum(latent_codes_2 ** 2, axis=-1)\n",
    "    dot = tf.reduce_sum(latent_codes_1 * latent_codes_2, axis=-1)\n",
    "    return dot / tf.sqrt(sq_norm_1 * sq_norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:39: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:42: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:47: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, merge\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users, latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items, latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = merge([user_embedding, positive_item_embedding],\n",
    "                                mode=cos_mode, output_shape=(1,),\n",
    "                                name=\"positive_similarity\")\n",
    "    negative_similarity = merge([user_embedding, negative_item_embedding],\n",
    "                                mode=cos_mode, output_shape=(1,),\n",
    "                                name=\"negative_similarity\")\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = merge([positive_similarity, negative_similarity],\n",
    "                         mode=margin_comparator_loss, output_shape=(1,),\n",
    "                         name='comparator_loss')\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                 positive_item_input,\n",
    "                                 negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n",
    "\n",
    "triplet_model, match_model = build_models(n_users, n_items, latent_dim=64, l2_reg=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "user_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "positive_item_input (InputLayer) (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "user_embedding (Embedding)       (None, 1, 64)         60416       user_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "item_embedding (Embedding)       (None, 1, 64)         107712      positive_item_input[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 64)            0           user_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 64)            0           item_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "positive_similarity (Merge)      (None, 1)             0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 168,128\n",
      "Trainable params: 168,128\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(match_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "user_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "positive_item_input (InputLayer) (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "negative_item_input (InputLayer) (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "user_embedding (Embedding)       (None, 1, 64)         60416       user_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "item_embedding (Embedding)       (None, 1, 64)         107712      positive_item_input[0][0]        \n",
      "                                                                   negative_item_input[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 64)            0           user_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 64)            0           item_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 64)            0           item_embedding[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "positive_similarity (Merge)      (None, 1)             0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "negative_similarity (Merge)      (None, 1)             0           flatten_1[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "comparator_loss (Merge)          (None, 1)             0           positive_similarity[0][0]        \n",
      "                                                                   negative_similarity[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 168,128\n",
      "Trainable params: 168,128\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(triplet_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def average_roc_auc(match_model, data_train, data_test):\n",
    "    \"\"\"Compute the ROC AUC for each user and average over users\"\"\"\n",
    "    max_user_id = max(data_train['user_id'].max(), data_test['user_id'].max())\n",
    "    max_item_id = max(data_train['item_id'].max(), data_test['item_id'].max())\n",
    "    user_auc_scores = []\n",
    "    for user_id in range(1, max_user_id + 1):\n",
    "        pos_item_train = data_train[data_train['user_id'] == user_id]\n",
    "        pos_item_test = data_test[data_test['user_id'] == user_id]\n",
    "        \n",
    "        # Consider all the items already seen in the training set\n",
    "        all_item_ids = np.arange(1, max_item_id + 1)\n",
    "        items_to_rank = np.setdiff1d(all_item_ids, pos_item_train['item_id'].values)\n",
    "        \n",
    "        # Ground truth: return 1 for each item positively present in the test set\n",
    "        # and 0 otherwise.\n",
    "        expected = np.in1d(items_to_rank, pos_item_test['item_id'].values)\n",
    "        \n",
    "        if np.sum(expected) >= 1:\n",
    "            # At least one positive test value to rank\n",
    "            repeated_user_id = np.empty_like(items_to_rank)\n",
    "            repeated_user_id.fill(user_id)\n",
    "\n",
    "            predicted = match_model.predict([repeated_user_id, items_to_rank],\n",
    "                                            batch_size=4096)\n",
    "            user_auc_scores.append(roc_auc_score(expected, predicted))\n",
    "\n",
    "    return sum(user_auc_scores) / len(user_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49961645600763538"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_roc_auc(match_model, pos_data_train, pos_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_triplets(pos_data, max_item_id, random_seed=0):\n",
    "    \"\"\"Sample negatives at random\"\"\"\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    user_ids = pos_data['user_id'].values\n",
    "    pos_item_ids = pos_data['item_id'].values\n",
    "\n",
    "    neg_item_ids = rng.randint(low=1, high=max_item_id + 1,\n",
    "                               size=len(user_ids))\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3s - loss: 0.7703\n",
      "Epoch 1/15: test ROC AUC: 0.8500\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3843\n",
      "Epoch 2/15: test ROC AUC: 0.8846\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3556\n",
      "Epoch 3/15: test ROC AUC: 0.8980\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3440\n",
      "Epoch 4/15: test ROC AUC: 0.9035\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3355\n",
      "Epoch 5/15: test ROC AUC: 0.9088\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3285\n",
      "Epoch 6/15: test ROC AUC: 0.9121\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3306\n",
      "Epoch 7/15: test ROC AUC: 0.9144\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3245\n",
      "Epoch 8/15: test ROC AUC: 0.9146\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3230\n",
      "Epoch 9/15: test ROC AUC: 0.9151\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3208\n",
      "Epoch 10/15: test ROC AUC: 0.9157\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3189\n",
      "Epoch 11/15: test ROC AUC: 0.9163\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3193\n",
      "Epoch 12/15: test ROC AUC: 0.9172\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3182\n",
      "Epoch 13/15: test ROC AUC: 0.9174\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3218\n",
      "Epoch 14/15: test ROC AUC: 0.9193\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3171\n",
      "Epoch 15/15: test ROC AUC: 0.9199\n"
     ]
    }
   ],
   "source": [
    "# we plug the identity loss and the a fake target variable ignored by\n",
    "# the model to be able to use the Keras API to train the triplet model\n",
    "triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "fake_y = np.ones_like(pos_data_train['user_id'])\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    triplet_inputs = sample_triplets(pos_data_train, max_item_id,\n",
    "                                     random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                      batch_size=64, epochs=1, verbose=2)\n",
    "    \n",
    "    # Monitor the convergence of the model\n",
    "    test_auc = average_roc_auc(match_model, pos_data_train, pos_data_test)\n",
    "    print(\"Epoch %d/%d: test ROC AUC: %0.4f\"\n",
    "          % (i + 1, n_epochs, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:66: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/taemyung/anaconda/envs/py35/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3s - loss: 0.4693\n",
      "Epoch 1/15: test ROC AUC: 0.8548\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3791\n",
      "Epoch 2/15: test ROC AUC: 0.8635\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3728\n",
      "Epoch 3/15: test ROC AUC: 0.8650\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3666\n",
      "Epoch 4/15: test ROC AUC: 0.8652\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3624\n",
      "Epoch 5/15: test ROC AUC: 0.8691\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3519\n",
      "Epoch 6/15: test ROC AUC: 0.8779\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3374\n",
      "Epoch 7/15: test ROC AUC: 0.8882\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3210\n",
      "Epoch 8/15: test ROC AUC: 0.8918\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3135\n",
      "Epoch 9/15: test ROC AUC: 0.8948\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3050\n",
      "Epoch 10/15: test ROC AUC: 0.8956\n",
      "Epoch 1/1\n",
      "2s - loss: 0.2997\n",
      "Epoch 11/15: test ROC AUC: 0.8991\n",
      "Epoch 1/1\n",
      "2s - loss: 0.2958\n",
      "Epoch 12/15: test ROC AUC: 0.9025\n",
      "Epoch 1/1\n",
      "2s - loss: 0.2883\n",
      "Epoch 13/15: test ROC AUC: 0.9051\n",
      "Epoch 1/1\n",
      "2s - loss: 0.2864\n",
      "Epoch 14/15: test ROC AUC: 0.9079\n",
      "Epoch 1/1\n",
      "2s - loss: 0.2766\n",
      "Epoch 15/15: test ROC AUC: 0.9115\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/deep_implicit_feedback_recsys.py\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout, merge\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"Build the shared multi layer perceptron\"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"Build models to train a deep triplet network\"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = concatenate([user_embedding, positive_item_embedding],\n",
    "                                     name=\"positive_embeddings_pair\")\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "    negative_embeddings_pair = concatenate([user_embedding, negative_item_embedding],\n",
    "                                     name=\"negative_embeddings_pair\")\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = merge([positive_similarity, negative_similarity],\n",
    "                         mode=margin_comparator_loss, output_shape=(1,),\n",
    "                         name='comparator_loss')\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                      positive_item_input,\n",
    "                                      negative_item_input],\n",
    "                               outputs=triplet_loss)\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                             outputs=positive_similarity)\n",
    "\n",
    "    return deep_match_model, deep_triplet_model\n",
    "\n",
    "hyper_parameters = dict(\n",
    "    user_dim=32,\n",
    "    item_dim=64,\n",
    "    n_hidden=1,\n",
    "    hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    l2_reg=0\n",
    ")\n",
    "\n",
    "deep_match_model, deep_triplet_model = build_models(n_users, n_items,**hyper_parameters)\n",
    "\n",
    "deep_triplet_model.compile(loss=identity_loss, optimizer='adam')\n",
    "fake_y = np.ones_like(pos_data_train['user_id'])\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    triplet_inputs = sample_triplets(pos_data_train, max_item_id, random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                           batch_size=64, epochs=1, verbose=2)\n",
    "\n",
    "    # Monitor the convergence of the model\n",
    "    test_auc = average_roc_auc(deep_match_model, pos_data_train, pos_data_test)\n",
    "    print(\"Epoch %d/%d: test ROC AUC: %0.4f\" % (i + 1, n_epochs, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
